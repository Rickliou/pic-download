#!/usr/bin/env python3
"""
18comic ç›¸ç°¿æ‰¹é‡ä¸‹è¼‰å·¥å…·

å¾ç›¸ç°¿åˆ—è¡¨é é¢æå–æ‰€æœ‰ç« ç¯€é€£çµï¼Œä¾åºä¸‹è¼‰ä¸¦ç”Ÿæˆ PDFã€‚
"""
import argparse
import re
import sys
import time
import random
from pathlib import Path
from dataclasses import dataclass

from playwright.sync_api import sync_playwright, Page, Browser
from descrambler import restore_image
from scraper import get_random_user_agent, download_image, _scroll_page
from to_pdf import images_to_pdf


def sanitize_filename(name: str) -> str:
    """
    æ¸…ç†æª”å/ç›®éŒ„åç¨±ï¼Œé˜²æ­¢è·¯å¾‘éæ­·æ”»æ“Šã€‚
    
    Args:
        name: åŸå§‹åç¨±
    
    Returns:
        str: æ¸…ç†å¾Œçš„å®‰å…¨åç¨±
    """
    # ç§»é™¤è·¯å¾‘éæ­·ç¬¦è™Ÿ
    name = name.replace('..', '')
    # ç§»é™¤æ‰€æœ‰æ–œç·š
    name = name.replace('/', '_').replace('\\', '_')
    # ç§»é™¤ç‰¹æ®Šå­—å…ƒ
    name = re.sub(r'[<>:"|?*]', '_', name)
    # ç§»é™¤é–‹é ­çš„é»å’Œç©ºæ ¼
    name = name.lstrip('. ')
    # ç§»é™¤çµå°¾çš„é»å’Œç©ºæ ¼
    name = name.rstrip('. ')
    # é™åˆ¶é•·åº¦
    name = name[:200] if name else "untitled"
    # å¦‚æœæ¸…ç†å¾Œç‚ºç©ºï¼Œè¿”å›é è¨­å€¼
    return name if name else "untitled"


@dataclass
class ChapterInfo:
    """ç« ç¯€è³‡è¨Š"""
    title: str
    url: str
    photo_id: str
    episode_num: int  # è©±æ•¸ç·¨è™Ÿ


def extract_album_chapters(url: str, headless: bool = True) -> tuple[str, list[ChapterInfo]]:
    """
    å¾ç›¸ç°¿é é¢æå–æ‰€æœ‰ç« ç¯€é€£çµã€‚
    
    Args:
        url: ç›¸ç°¿é é¢ URLï¼ˆå¦‚ https://18comic.vip/album/1223474/ï¼‰
        headless: æ˜¯å¦ä½¿ç”¨ç„¡é ­æ¨¡å¼
    
    Returns:
        tuple[str, list[ChapterInfo]]: (ç›¸ç°¿æ¨™é¡Œ, ç« ç¯€åˆ—è¡¨)
    """
    with sync_playwright() as p:
        browser: Browser = p.chromium.launch(headless=headless)
        context = browser.new_context(
            user_agent=get_random_user_agent(),
            viewport={"width": 1920, "height": 1080},
        )
        
        page: Page = context.new_page()
        page.set_extra_http_headers({
            "Referer": "https://18comic.vip/",
            "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        })
        
        # è¨ªå•é é¢
        page.goto(url, wait_until="domcontentloaded", timeout=60000)
        
        # ç­‰å¾…ä¸¦æ²å‹•ä»¥è¼‰å…¥æ‰€æœ‰å…§å®¹
        page.wait_for_timeout(2000)
        _scroll_page(page, scroll_times=5, delay_ms=500)
        
        # æå–ç›¸ç°¿æ¨™é¡Œ
        album_title = page.evaluate("""
            () => {
                const h1 = document.querySelector('h1');
                return h1 ? h1.textContent.trim() : 'Unknown';
            }
        """)
        
        # æå–æ‰€æœ‰ç« ç¯€é€£çµ
        raw_chapters = page.evaluate("""
            () => {
                const links = Array.from(document.querySelectorAll('a[href*="/photo/"]'));
                const seen = new Set();
                const results = [];
                
                for (const a of links) {
                    const href = a.getAttribute('href');
                    const text = a.textContent.trim();
                    
                    // è·³éé‡è¤‡é€£çµ
                    if (seen.has(href)) continue;
                    seen.add(href);
                    
                    // è·³éç©ºæ–‡å­—æˆ–å¤ªçŸ­çš„é€£çµï¼ˆå¯èƒ½æ˜¯æŒ‰éˆ•ï¼‰
                    if (!text || text.length < 2) continue;
                    
                    results.push({ text, href });
                }
                
                return results;
            }
        """)
        
        browser.close()
    
    # è§£æç« ç¯€è³‡è¨Š
    chapters = []
    episode_counter = 0
    
    for item in raw_chapters:
        text = item["text"]
        href = item["href"]
        
        # éæ¿¾ä¼‘åˆŠå…¬å‘Š
        if "ä¼‘åˆŠ" in text or "å…¬å‘Š" in text:
            print(f"  â­ï¸  è·³é: {text}")
            continue
        
        # æå– photo_id
        match = re.search(r'/photo/(\d+)', href)
        if not match:
            continue
        
        photo_id = match.group(1)
        episode_counter += 1
        
        # å»ºç«‹å®Œæ•´ URL
        full_url = f"https://18comic.vip/photo/{photo_id}"
        
        chapters.append(ChapterInfo(
            title=text,
            url=full_url,
            photo_id=photo_id,
            episode_num=episode_counter,
        ))
    
    return album_title, chapters


def download_chapter_images(
    chapter: ChapterInfo,
    output_dir: Path,
    headless: bool = True,
    delay: float = 0.3,
) -> Path:
    """
    ä¸‹è¼‰å–®ä¸€ç« ç¯€çš„æ‰€æœ‰åœ–ç‰‡ã€‚
    
    Args:
        chapter: ç« ç¯€è³‡è¨Š
        output_dir: è¼¸å‡ºç›®éŒ„
        headless: æ˜¯å¦ä½¿ç”¨ç„¡é ­æ¨¡å¼
        delay: ä¸‹è¼‰é–“éš”
    
    Returns:
        Path: åœ–ç‰‡å„²å­˜ç›®éŒ„
    """
    from scraper import scrape_album
    
    # å»ºç«‹ç« ç¯€ç›®éŒ„
    chapter_dir = output_dir / f"ep{chapter.episode_num:03d}_{chapter.photo_id}"
    chapter_dir.mkdir(exist_ok=True)
    
    print(f"\nğŸ“– ç¬¬ {chapter.episode_num} è©±: {chapter.title}")
    print(f"   URL: {chapter.url}")
    
    # çˆ¬å–åœ–ç‰‡åˆ—è¡¨
    album = scrape_album(chapter.url, headless=headless)
    print(f"   æ‰¾åˆ° {len(album.images)} å¼µåœ–ç‰‡")
    
    # ä¸‹è¼‰ä¸¦é‚„åŸæ¯å¼µåœ–ç‰‡
    for img_info in album.images:
        output_filename = f"{img_info.index:04d}_{img_info.photo_id}.webp"
        output_path = chapter_dir / output_filename
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if output_path.exists():
            continue
        
        try:
            # ä¸‹è¼‰ä¸¦é‚„åŸ
            scrambled_data = download_image(img_info.url, referer=chapter.url)
            restored_img = restore_image(scrambled_data, album.aid, img_info.photo_id)
            restored_img.save(output_path)
            print(f"   âœ“ {img_info.photo_id}")
        except Exception as e:
            print(f"   âœ— {img_info.photo_id}: {e}")
        
        time.sleep(delay + random.uniform(0, 0.2))
    
    return chapter_dir


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(
        description="18comic ç›¸ç°¿æ‰¹é‡ä¸‹è¼‰å·¥å…·",
    )
    parser.add_argument(
        "album_url",
        help="ç›¸ç°¿é é¢ URLï¼ˆå¦‚ https://18comic.vip/album/1223474/ï¼‰",
    )
    parser.add_argument(
        "--output-dir", "-o",
        default="./output",
        help="è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­ï¼š./outputï¼‰",
    )
    parser.add_argument(
        "--headless/--no-headless",
        dest="headless",
        action=argparse.BooleanOptionalAction,
        default=True,
        help="æ˜¯å¦ä½¿ç”¨ç„¡é ­ç€è¦½å™¨æ¨¡å¼",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=0.3,
        help="æ¯å¼µåœ–ç‰‡ä¸‹è¼‰é–“éš”ç§’æ•¸ï¼ˆé è¨­ï¼š0.3ï¼‰",
    )
    parser.add_argument(
        "--start-from",
        type=int,
        default=1,
        help="å¾ç¬¬å¹¾è©±é–‹å§‹ä¸‹è¼‰ï¼ˆé è¨­ï¼š1ï¼‰",
    )
    parser.add_argument(
        "--end-at",
        type=int,
        default=None,
        help="ä¸‹è¼‰åˆ°ç¬¬å¹¾è©±çµæŸï¼ˆé è¨­ï¼šå…¨éƒ¨ï¼‰",
    )
    
    args = parser.parse_args()
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ” æ­£åœ¨è§£æç›¸ç°¿: {args.album_url}")
    
    try:
        # æå–ç« ç¯€åˆ—è¡¨
        album_title, chapters = extract_album_chapters(args.album_url, headless=args.headless)
        
        print(f"\nğŸ“š ç›¸ç°¿æ¨™é¡Œ: {album_title}")
        print(f"ğŸ“– å…± {len(chapters)} å€‹ç« ç¯€ï¼ˆå·²éæ¿¾ä¼‘åˆŠå…¬å‘Šï¼‰")
        
        if not chapters:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ç« ç¯€")
            sys.exit(1)
        
        # å»ºç«‹ç›¸ç°¿ç›®éŒ„
        # æ¸…ç†æ¨™é¡Œä¸­çš„ç‰¹æ®Šå­—å…ƒï¼ˆé˜²æ­¢è·¯å¾‘éæ­·æ”»æ“Šï¼‰
        safe_title = sanitize_filename(album_title)
        album_dir = output_dir / safe_title
        album_dir.mkdir(exist_ok=True)
        
        images_dir = album_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        pdf_dir = album_dir / "pdf"
        pdf_dir.mkdir(exist_ok=True)
        
        # ç¯©é¸è¦ä¸‹è¼‰çš„ç« ç¯€
        chapters_to_download = [
            ch for ch in chapters
            if ch.episode_num >= args.start_from
            and (args.end_at is None or ch.episode_num <= args.end_at)
        ]
        
        print(f"\nâ¬‡ï¸  å°‡ä¸‹è¼‰ {len(chapters_to_download)} å€‹ç« ç¯€")
        print(f"   åœ–ç‰‡ç›®éŒ„: {images_dir}")
        print(f"   PDF ç›®éŒ„: {pdf_dir}")
        
        # ä¾åºä¸‹è¼‰æ¯å€‹ç« ç¯€
        for chapter in chapters_to_download:
            try:
                # ä¸‹è¼‰åœ–ç‰‡
                chapter_image_dir = download_chapter_images(
                    chapter,
                    images_dir,
                    headless=args.headless,
                    delay=args.delay,
                )
                
                # ç”Ÿæˆ PDF
                pdf_filename = f"ep{chapter.episode_num:03d}.pdf"
                pdf_path = pdf_dir / pdf_filename
                
                if not pdf_path.exists():
                    print(f"   ğŸ“„ ç”Ÿæˆ PDF: {pdf_filename}")
                    images_to_pdf(chapter_image_dir, pdf_path)
                
            except KeyboardInterrupt:
                print("\n\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·")
                sys.exit(1)
            except Exception as e:
                print(f"   âŒ éŒ¯èª¤: {e}")
                continue
            
            # ç« ç¯€é–“å»¶é²
            time.sleep(1)
        
        print(f"\nğŸ‰ å®Œæˆï¼")
        print(f"   åœ–ç‰‡: {images_dir}")
        print(f"   PDF:  {pdf_dir}")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
